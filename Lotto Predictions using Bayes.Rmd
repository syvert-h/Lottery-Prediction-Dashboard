---
title: "Lottery Predictions using Bayes Theorem"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(gridExtra)
```

## Summary

The whole idea behind Bayes Theorem is to update prior beliefs based on new evidence, in generate probabilities which represent of our new degree of belief.

In context of the lottery, we had prior beliefs that each ball drawn was fair (so probabilities came from a uniform distribution). Using over 1500 observations from past draws, we used this as evidence to update our prior degree of belief.

If the lottery was truly fair, we expect the Bayesian models to perform similarly to a random model  (uniform distribution).

An important aspect of Bayes Theorem is the likelihood function. This function controls how much we change our prior degree of belief. It is important this function is representative of the data-generating process to ensure valid statistical inference.

- Binomial: For each ball, each number is or is not drawn -- Several trials of two outcomes for each ball number; the Binomial function can be appropriate.

- Observed Proportions: For each ball, each number has a certain frequency of occurring -- Convert this frequency to a proportion and we have the exact probability of each ball number from the past -- If this exact behaviour was the true underlying distribution, observed proportions could be appropriate.

- Inverse Proportions: This is the inverse of observed frequencies

For the Bayesian models, it was assumed that evidence gathered for the likelihood function was INDEPENDENT from the previous balls drawn (NOT RESTRICTED to specific past observations).

Predictions are generated from random sampling based on posterior probabilities. This was chosen to give all balls a chance at being selected, just like in a real lottery.

After much testing for each model:

#### There was no clear cut model which performed better than others. The results from all models were inconsistent.

Trying to predict numbers using Bayes Theorem did not see any improvement.

#### At the end of the day, winning Lotto/Strike is a just a very unlikely coincidence!

## Setup & Data Cleaning
```{r echo=F}
# Read-in data
data = read_xlsx("official-lotto-results.xlsx", skip=5) # latest draws
# Data Info
summary(data)
# Note: if NA in data, would show  in column summary
```

The latest six draws:

```{r echo=F}
# Clean the data
clean_df = function(df) {
  df %>%
    rename(`Date` = `Draw Date`, `7` = `Bonus Ball`, `PB` = `Power Ball`) %>%
    mutate(
      `Date` = as.Date(`Date`, format = "%A %d %B %Y"), # convert to date
      `2nd Bonus Ball` = NULL, # remove column
    ) %>%
    mutate(across(where(is.double) & !where(is.Date), as.integer)) # convert doubles to integer
  
  # Note: PB contains some NA (didn't exist until later)
}
lotto = clean_df(data)
head(lotto)
# write.csv(lotto, "./LotteryApp/data/lotto_clean.csv", row.names=F)
```

## Data Exploration
```{r echo=F}
# Plot functions
line_dot_plot = function(df, xvar, yvar) {
  df %>%
    ggplot(aes(x=!!sym(xvar), y=!!sym(yvar))) +
    geom_point() + geom_line() +
    geom_smooth() + theme_minimal()
}

prob_bar_plot = function(df, xvar, p) {
  df %>%
    ggplot(aes(x=!!sym(xvar))) +
    geom_bar(aes(y=after_stat(count / sum(count)))) +
    geom_hline(yintercept=p, color="red", linewidth=1) +
    labs(y="Probability")
}
```

#### Ball 1
```{r echo=F}
line_dot_plot(lotto, "Draw", "1")
```

There appears to be a period of time when the number drawn for Ball 1 was unusually low. It is best we avoid these values to reduce any biases.

```{r echo=F}
lotto %>%
  filter(`Draw` > 600) %>%
  line_dot_plot("Draw", "1")
```

Better. There doesn't appear to be any unusual patterns. An expected value (mean) of close to 20 over time indicates that the distribution may be uniform (E[X] = (N+1)/2 where N is the number of items). In a uniform distribution, all N items have the same probability of occurring (p = 1/N = 1/40 (2.5%)).

```{r echo=F}
lotto %>%
  filter(`Draw` > 600) %>%
  prob_bar_plot("1", p=1/40)
```

All values have close to a 1/40 probability of occurring. The reason some values occur more than others may simply be due to the 'roll of the dice' i.e. randomness.

#### Other Balls BEFORE filtering
```{r echo=F}
# Graphs for Ball 2-6
for (i in 2:6) {
  p = 1/(41-i)
  i = as.character(i)
  p1 = line_dot_plot(lotto, "Draw", i)
  p2 = prob_bar_plot(lotto, i, p)
  grid.arrange(p1, p2, ncol=2)
}
```

Update the dataset accordingly.

```{r}
lotto = lotto %>% filter(`Draw` > 600)
```

#### Other Balls AFTER filtering

```{r echo=F}
# Graphs for Ball 2-6
for (i in 2:6) {
  p = 1/(41-i)
  i = as.character(i)
  p1 = line_dot_plot(lotto, "Draw", i)
  p2 = prob_bar_plot(lotto, i, p)
  grid.arrange(p1, p2, ncol=2)
}
```

## Model Building

Assuming the Lottery is fair, each number for the first ball has a 1/40 (2.5%) chance of being selected. But we saw earlier there may be some evidence that some numbers occur more often that others.

### Model Choice: Bayes Theorem

I am using a Bayesian model. With this new information, we can update prior beliefs that the lottery is fair, with what we observe.

If what we observe is fair, then our new (posterior) probabilities should not deviate too far than expected. Otherwise, posterior probabilities should tend towards balls which are more likely to occur.

### Example - Bayes Theorem for the First Ball:

#### Hypothesis and Prior (P(H))

Aim: Determine the probability of each ball number being drawn first.

Assuming the lottery is fair, each ball should have a 1/40 probability of occurring. This will be our prior belief for the first ball drawn: P(H) = 1/40 (2.5%).

#### Evidence

We have a history about all balls drawn in the lotto. This is our evidence to update prior beliefs.

```{r echo=F}
ball1_freq = table(lotto$`1`) # counts
ball1_prop = prop.table(ball1_freq) # counts as a proportion
ball1_prop
```

Each probability represent how often each ball number occurs for the first ball drawn.

#### Likelihood (P(E|H))

The likelihood function plays a critical role in generating posterior probabilities. It is important the function chosen is representative of how our data is collected.

- Binomial Probability Function
  - Each lotto draw is an independent trial.
  - Each ball number has a binary outcome: it does or doesn't occur.
  - So when drawing the 1st (or any) ball, we have multiple Bernoulli outcomes for each ball number --> Binomial probability function is appropriate.

- Observed Proportions
  - Each ball number occupies a certain proportion of our observed frequencies.
  - Observed proportions represent the exact probability of each ball number occurring to date --> Observed proportions can be appropriate.

- Inverse Proportions
  - Similar to observed proportions, instead take the inverse probability of each ball number.
  - Balls which occur more frequently are given less weight; Balls which occur less frequently are given more weight.
  - Idea: If the lottery is truly fair, then these probabilities should be adjusted to converge toward the expected value (e.g. 1/40 for the first ball).
  
For example, we implement the Binomial probability function where K := observed frequency for each number, p := 1/40, and N := number of different draws.

```{r}
# Likelihood of EACH first ball (using binomial probability formula)
n = length(lotto$`1`) # number of trials
k = ball1_freq # number of successes (from n trials)
prior = 1/40 # assuming fair
likelihood = choose(n, k) * (prior)^k * (1 - prior)^(n-k) # P(E|H)
likelihood
```

#### Posterior (P(H\|E))

Now, scale the values produced from the likelihood function to represent our new probabilities for each ball number.

```{r}
# posterior probabilities (using Bayes Theorem)
posterior = (prior * likelihood) / sum(prior * likelihood)
# Note: sum(likelihood) is literally P(E) := probability of our evidence -- without this we would have our proportion
posterior
```

Visualising the posterior probabilities:

```{r echo=F}
# visualise posterior probabilities
data.frame("Post"=posterior, "Prop"=ball1_prop) %>%
  select(c(2,4)) %>%
  rename("Posterior"="Post.Freq", "Proportion"="Prop.Freq") %>%
  ggplot(aes(x=1:40, y=`Proportion`)) +
  geom_col(fill="black", alpha=0.2) + # observed proportions
  geom_col(aes(y=`Posterior`), fill="blue", alpha=0.2) + # posterior
  geom_hline(yintercept=1/40, color="red") +
  labs(y="Probability", x="Ball 1")
```

#### Predictions

Because the Lottery is random in nature, I feel it's best to randomly select the first ball using probabilities from the posterior. This way all balls have a chance of being selected, much like in Lotto.

## Generalising the Bayes Theorem Process

The following code generalises the Bayes Theorem process so that it works with Balls 1 to 6. It also accounts for different likelihood functions.

```{r}
##### Bayes Theorem #####
general_bayes = function(cols, pb, ll, prev=NULL) {
  ### Prep ###
  ball_i = 1 # i_th ball to predict
  nums = do.call(c, cols) # convert into one vector
  if (!is.null(prev)) { # filter out balls already drawn
    nums = nums[!(nums %in% prev)]
    ball_i = length(prev)+1
  }
  ### Evidence ###
  prior = ifelse(pb == T, 1/10, 1/(41-ball_i)) # prior (fair) probability
  counts = table(nums)
  props = prop.table(counts)
  ### Likelihood ###
  N = sum(counts) # number of trials
  k = counts # number of successes (being drawn) for each ball number
  likelihoods = list(
    "prop" = props,
    "invert.prop" = (1-props)/sum((1-props)),
    "binom" = dbinom(x=k, size=N, prob=prior)
  )
  likelihood = likelihoods[[ll]]
  ### Posterior ###
  prob_h_given_e = prior * likelihood
  posterior = prob_h_given_e / sum(prob_h_given_e)
  ### Predictions ###
  balls = 1:40
  if(pb == T) {balls = 1:10}
  if (!is.null(prev)) {balls = balls[-prev]}
  return(sample(balls, size=1, prob=posterior)) # returns 1 ball
}

# One line of Bayesian Predictions
bayes_line = function(df, nBalls, pb, ll) {
  preds_line = numeric(nBalls)
  prev = NULL
  if (nBalls == 4) {
    # Note: using for-loop since apply-loops cannot alter variables outside its scope/function
    for (i in 1:nBalls) {
      pred_num = general_bayes(cols=df[i], prev=prev, pb=pb, ll=ll)
      preds_line[i] = pred_num
      if (is.null(prev)) {prev = pred_num} else {prev = c(prev, pred_num)}
    }
  } else { # Lotto or PB
    for (i in 1:nBalls) {
      pred_num = general_bayes(cols=df, prev=prev, pb=pb, ll=ll)
      preds_line[i] = pred_num
      if (is.null(prev)) {prev = pred_num} else {prev = c(prev, pred_num)}
    }
  }
  return(preds_line)
}

# One line of predictions using Uniform sampling
rand_line = function(N=4) {sample(1:40, size=N)}
```

## Posterior Distributions

Using the latest draw, sample repeatedly to get an idea of how each model behaves.

```{r}
strike_df = lotto %>% select(1, 3:6)
lotto_df = lotto %>% select(1, 3:8)
nTimes = 2000

prop_bayes = replicate(nTimes, bayes_line(df=strike_df[,-1], nBalls=4, pb=F, ll='prop'))
invert_prop_bayes = replicate(nTimes, bayes_line(df=strike_df[,-1], nBalls=4, pb=F, ll='invert.prop'))
binom_bayes = replicate(nTimes, bayes_line(df=strike_df[,-1], nBalls=4, pb=F, ll='binom'))
rand_latest = replicate(nTimes, rand_line(N=4))
```

Visualing predictions for each ball drawn:

```{r echo=F}
prob_bar_plot2 = function(vec, ball_i, model) {
  props = prop.table(table(vec))
  props %>% 
    as_tibble() %>%
    rename("Ball No." = "vec", "Probability"="n") %>%
    mutate(`Ball No.` = as.integer(`Ball No.`)) %>%
    arrange(`Ball No.`) %>%
    ggplot(aes(x=`Ball No.`, y=`Probability`)) +
    geom_bar(stat='identity') +
    geom_hline(yintercept = 1/(41-ball_i), color="red") +
    labs(title=sprintf('%s Ball %s', model, ball_i))
}

for (i in 1:4) {
  p1 = prob_bar_plot2(vec=prop_bayes[i,], ball_i=i, model='Proportion')
  p2 = prob_bar_plot2(vec=invert_prop_bayes[i,], ball_i=i, model='Invert Proportion')
  p3 = prob_bar_plot2(vec=binom_bayes[i,], ball_i=i, model='Binomial')
  p4 = prob_bar_plot2(vec=rand_latest[i,], ball_i=i, model='Random/Uniform')
  grid.arrange(p1, p2, p3, p4, ncol=2, nrow=2)
}
```

Note: Better convergence when sampling 10,000 times. However due to time constraint, sampling 2000 gives a decent idea of how each probability model performs.

## Model Performance

Going forward, when using a Bayesian model I do NOT assume independence between the balls drawn (i.e. do not restrict to only past sequences). Reason being that there are few cases when drawing balls later on and we basically end up sampling from a uniform distribution. Since we already have a uniform model, this behaviour should already be captured.

### Accuracy Measures

For Lotto, a ticket is typically composed of ~10 lines. For strike, it is usually an addition to Lotto where ~2 lines are played. It is best to measure each model's predictive accuracy according to how it is played in real life!

I use the latest 1% of draws as test data (a.t.m. ~17 draws).

```{r warning=F}
### Strike Predictive Accuracy  ###
get_draw_preds = function(drawNo, nBalls) {
  temp_df = lotto %>% filter(`Draw` < drawNo)
  temp_df2 = temp_df %>% select(3:8) # Lotto
  nTimes = 10
  if (nBalls == 4) { # Strike
    temp_df2 = temp_df2 %>% select(1:4)
    nTimes = 2
  }
  
  prop_preds = replicate(nTimes, {bayes_line(df=temp_df2, nBalls=nBalls, pb=F, ll='prop')}) %>% t() %>%
  as_tibble() %>% mutate(`Model` = 'Proportion')
  invert_prop_preds = replicate(nTimes, {bayes_line(df=temp_df2, nBalls=nBalls, pb=F, ll='invert.prop')}) %>% t() %>%
    as_tibble() %>% mutate(`Model` = 'Invert Proportion')
  binom_preds = replicate(nTimes, {bayes_line(df=temp_df2, nBalls=nBalls, pb=F, ll='binom')}) %>% t() %>%
    as_tibble() %>% mutate(`Model` = 'Binomial')
  rand_preds = replicate(nTimes, {rand_line(N=nBalls)}) %>% t() %>%
     as_tibble() %>% mutate(`Model` = 'Uniform/Random')

  return(bind_rows(prop_preds, invert_prop_preds, binom_preds, rand_preds) %>%
           mutate(`DrawNo` = drawNo))
}

numTestDraws = round(nrow(lotto) * 0.01) # 1% of draws
latestDraws = seq(to=max(lotto$Draw), length=numTestDraws) # latest 1% of draws
latest_preds_1 = lapply(latestDraws, get_draw_preds, nBalls=4) %>%
  bind_rows()
```

We are using the latest 1% of draws as the performance on more recent draws is of more importance to us. The following combines acutal results to the our generated prediction dataframe.

```{r}
# Prepare predictions dataframe
temp_lotto = lotto %>% filter(`Draw` >= min(latestDraws))

get_merged_preds_df = function(preds_df) { # returns preds_df with acutal results combined
  preds_df %>%
    inner_join(temp_lotto[,c("Draw","1","2","3","4")], by=join_by("DrawNo" == "Draw")) %>%
    mutate(
      "Correct 1" = `V1` == `1`,
      "Correct 2" = `V2` == `2`,
      "Correct 3" = `V3` == `3`,
      "Correct 4" = `V4` == `4`
    )
}
merged_latest_preds_1 = get_merged_preds_df(latest_preds_1)
```

#### Accuracy per Ball Order - Singular Predictions

```{r}
# Per Ball Order Accuracy
get_per_ball_order_accuracy = function(merged_df) {
  merged_df %>%
    group_by(`Model`) %>%
    reframe(
      "% Correct 1" = mean(`Correct 1`)*100,
      "% Correct 2" = mean(`Correct 2`)*100,
      "% Correct 3" = mean(`Correct 3`)*100,
      "% Correct 4" = mean(`Correct 4`)*100,
    ) %>%
    rowwise() %>%
    mutate(
      "Average" = mean(c(`% Correct 1`,`% Correct 2`,`% Correct 3`,`% Correct 4`)),
      "Variance" = var(c(`% Correct 1`,`% Correct 2`,`% Correct 3`,`% Correct 4`))
    )
}

get_per_ball_order_accuracy(merged_latest_preds_1)
```

## Conclusion

Resulting differences could be explained as randomness/luck of the draw.

Ultimately, using Bayes Theorem to adjust prior beliefs about the Lotto did not produce as much improvement. At the end of the day, winning the Lotto/Strike is a just a very unlikely coincidence!

## Future Improvements

1. Method of generating predictions: Sample many times then take the ball number with the highest probability.

- Pros:
  - Eliminates some randomness

- Cons: 
  - Possibly overfits to one number (happens more often in a biased model)
  - Time consuming to generate predictions

2. Introduce cross validation in model training:
- To generate posterior probabilities, we currently learn from all draws BEFORE the given draw.
  - Assumes a sequence in the data.
- To create a more robust model, introduce cross-validation in the model training phase.

3. Undertake a more in-depth process to creating a Bayesian model (if necessary).
- This analysis is designed to understand the core principle of Bayes Theorem.
- There is a whole subsection of statistics dedicated to Bayesian statistics. Eventually, I'd like to undertake this process to improve this analysis.

## Improvement 1 - Eliminate Randomness with Resampling:

Running the previous code for accuracy will return varying results due to the randomness involved in the number generating process.

To combat this, we sample many times before taking the most likely value from our sample!

Note: Sampling too many times (~10000) will produce convergence  among models, hence predictions will lack diversity.

### Most Likely Numbers from 1000 resamples:

```{r}
### Strike Predictive Accuracy - Resampled 1000 Times ###
get_draw_preds2 = function(drawNo, nBalls, nTimes=1000) {
  temp_df = lotto %>% filter(`Draw` < drawNo)
  temp_df2 = temp_df %>% select(3:8) # lotto
  if (nBalls == 4) {temp_df2 = temp_df2 %>% select(1:4)} # strike
  
  prop_preds = replicate(nTimes, {bayes_line(df=temp_df2, nBalls=nBalls, pb=F, ll='prop')}) %>% t() %>%
  as_tibble() %>% mutate(`Model` = 'Proportion')
  invert_prop_preds = replicate(nTimes, {bayes_line(df=temp_df2, nBalls=nBalls, pb=F, ll='invert.prop')}) %>% t() %>%
    as_tibble() %>% mutate(`Model` = 'Invert Proportion')
  binom_preds = replicate(nTimes, {bayes_line(df=temp_df2, nBalls=nBalls, pb=F, ll='binom')}) %>% t() %>%
    as_tibble() %>% mutate(`Model` = 'Binomial')
  rand_preds = replicate(nTimes, {rand_line(N=nBalls)}) %>% t() %>%
     as_tibble() %>% mutate(`Model` = 'Uniform/Random')

  return(bind_rows(prop_preds, invert_prop_preds, binom_preds, rand_preds) %>%
           mutate(`DrawNo` = drawNo))
}

latest_preds_1000 = lapply(latestDraws, get_draw_preds2, nBalls=4, nTimes=1000) %>%
  bind_rows()
# Each model produces 1000 predictions for every draw
```

Once we have generated predictions, get the most likely for each ball order.

```{r warning=F}
most_likely_model_preds = function(df, nBalls=4) { # given dataframe grouped by drawNo + model
  balls = numeric(nBalls)
  for (i in 1:nBalls) {
    ball_col = df[,i][!(df[,i] %in% balls)] # filer out chosen balls
    props = prop.table(table(ball_col))
    maxProp = max(props)
    possibleNums = names(props[props == maxProp])
    if (length(possibleNums) > 1) {
      possibleNums = sample(possibleNums, size=1) # randomly choose from equal props
    }
    balls[i] = possibleNums
  }
  return(balls)
}

most_likely_preds_1000 = latest_preds_1000 %>%
  group_by(`DrawNo`, `Model`) %>%
  do('Result' = most_likely_model_preds(df=., nBalls=4)) %>%
  separate(`Result`, into=c("Temp","V1","V2","V3","V4"), convert=T) %>%
  mutate(`Temp` = NULL)
# Prepare predictions dataframe
merged_most_likely_preds_1000 = get_merged_preds_df(most_likely_preds_1000)
```

#### Accuracy per Ball Order - Resample 1000 Times

```{r}
# Per Ball Order Accuracy
get_per_ball_order_accuracy(merged_most_likely_preds_1000)
```

Once again, model results are variable and can be attributed to the luck of the draw. However, selecting ball numbers using most likely method is better for its consistency; Singular predictions are very hit or miss (where you obviously miss much more often).

One improvement which could be tested is using different resample sizes for predictions. At the moment we are using 1000 resamples before selecting the most common ball. Trying different values may produce better results.

Note: We can try resample sizes >1000, but for the sake of time we only investigate up to 1000.

## Trying different resamples sizes

#### Resampling 10 times:

```{r warning=F}
latest_preds_10 = lapply(latestDraws, get_draw_preds2, nBalls=4, nTimes=10) %>%
  bind_rows()
most_likely_preds_10 = latest_preds_10 %>%
  group_by(`DrawNo`, `Model`) %>%
  do('Result' = most_likely_model_preds(df=., nBalls=4)) %>%
  separate(`Result`, into=c("Temp","V1","V2","V3","V4"), convert=T) %>%
  mutate(`Temp` = NULL)
merged_most_likely_preds_10 = get_merged_preds_df(most_likely_preds_10)
```

#### Accuracy per Ball Order - Resample 10 Times

```{r}
# Per Ball Order Accuracy
get_per_ball_order_accuracy(merged_most_likely_preds_10)
```

#### Resampling 50 times:

```{r warning=F}
latest_preds_50 = lapply(latestDraws, get_draw_preds2, nBalls=4, nTimes=50) %>%
  bind_rows()
most_likely_preds_50 = latest_preds_50 %>%
  group_by(`DrawNo`, `Model`) %>%
  do('Result' = most_likely_model_preds(df=., nBalls=4)) %>%
  separate(`Result`, into=c("Temp","V1","V2","V3","V4"), convert=T) %>%
  mutate(`Temp` = NULL)
merged_most_likely_preds_50 = get_merged_preds_df(most_likely_preds_50)
```

#### Accuracy per Ball Order - Resample 50 Times

```{r}
# Per Ball Order Accuracy
get_per_ball_order_accuracy(merged_most_likely_preds_50)
```

#### Resampling 100 times:

```{r warning=F}
latest_preds_100 = lapply(latestDraws, get_draw_preds2, nBalls=4, nTimes=100) %>%
  bind_rows()
most_likely_preds_100 = latest_preds_100 %>%
  group_by(`DrawNo`, `Model`) %>%
  do('Result' = most_likely_model_preds(df=., nBalls=4)) %>%
  separate(`Result`, into=c("Temp","V1","V2","V3","V4"), convert=T) %>%
  mutate(`Temp` = NULL)
merged_most_likely_preds_100 = get_merged_preds_df(most_likely_preds_100)
```

#### Accuracy per Ball Order - Resample 100 Times

```{r}
# Per Ball Order Accuracy
get_per_ball_order_accuracy(merged_most_likely_preds_100)
```

#### Resampling 500 times:

```{r warning=F}
latest_preds_500 = lapply(latestDraws, get_draw_preds2, nBalls=4, nTimes=500) %>%
  bind_rows()
most_likely_preds_500 = latest_preds_500 %>%
  group_by(`DrawNo`, `Model`) %>%
  do('Result' = most_likely_model_preds(df=., nBalls=4)) %>%
  separate(`Result`, into=c("Temp","V1","V2","V3","V4"), convert=T) %>%
  mutate(`Temp` = NULL)
merged_most_likely_preds_500 = get_merged_preds_df(most_likely_preds_500)
```

#### Accuracy per Ball Order - Resample 500 Times

```{r}
# Per Ball Order Accuracy
get_per_ball_order_accuracy(merged_most_likely_preds_500)
```

## Resampling Method Conclusion

It is difficult to gauge how models compare to one another based on different resample sizes because results are too inconsistent!

Keep in mind, the odds of predicting all four numbers correctly are 1 in 2,193,360. There is no saying in which prediction is better than others.

### Lotto results are simply just how the dice roll on the night!
